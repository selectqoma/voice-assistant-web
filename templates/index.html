<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Assistant Demo</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            margin: 2rem;
            line-height: 1.5;
        }
        h1 {
            margin-bottom: 1rem;
        }
        section {
            margin-bottom: 2rem;
        }
        label {
            display: block;
            margin-bottom: 0.5rem;
        }
        input[type="file"],
        input[type="text"] {
            margin-bottom: 0.5rem;
        }
        button {
            cursor: pointer;
            padding: 0.5rem 1rem;
            background-color: #007bff;
            color: white;
            border: none;
            border-radius: 4px;
        }
        button:disabled {
            background-color: #aaa;
        }
        .output {
            margin-top: 1rem;
            padding: 0.75rem;
            background-color: #f5f5f5;
            border-radius: 4px;
        }
    </style>
</head>
<body>
    <h1>Voice Assistant</h1>
    <section id="live-stt">
        <div style="margin-bottom: 0.5rem; display:flex; gap:0.75rem; align-items:center;">
            <label for="languageSelect" style="margin:0;">Language</label>
            <select id="languageSelect">
                <option value="auto" selected>Auto</option>
                <option value="en">English</option>
                <option value="fr">Français</option>
                <option value="nl">Nederlands</option>
            </select>
        </div>
        <div style="display:flex; gap:0.5rem; margin-bottom:0.5rem;">
            <button id="startBtn">Start</button>
            <button id="stopBtn" disabled>Stop</button>
        </div>
        <div id="indicators" style="display:flex; gap:1rem; align-items:center;">
            <span id="micState">Mic: idle</span>
            <span id="connState">WS: disconnected</span>
            <span id="botState">Bot: idle</span>
        </div>
        <div id="transcript" class="output" style="height: 220px; overflow:auto; margin-top:0.75rem;"></div>
        <audio id="ttsAudio" autoplay></audio>
    </section>

    <script>
        // Removed legacy upload/text chat; focusing on live voice-to-voice

        // Live STT with Soniox via direct WebSocket
        const startBtn = document.getElementById('startBtn');
        const stopBtn = document.getElementById('stopBtn');
        const micState = document.getElementById('micState');
        const connState = document.getElementById('connState');
        const botState = document.getElementById('botState');
        const transcriptDiv = document.getElementById('transcript');
        const languageSelect = document.getElementById('languageSelect');
        const ttsAudio = document.getElementById('ttsAudio');

        let mediaStream = null;
        let audioCtx = null;
        let scriptNode = null;
        let ws = null;
        let isRecording = false;
        let currentPartial = '';
        let finalSegments = [];
        let playingTts = false;
        let mediaSource = null;
        let sourceBuffer = null;
        let sbQueue = [];
        let endpointTimer = null;
        let lastFinalSent = '';

        function renderTranscript() {
            const finalText = finalSegments.map(s => s.text).join(' ');
            transcriptDiv.innerHTML = '';
            const finalEl = document.createElement('div');
            finalEl.textContent = finalText;
            finalEl.style.color = '#222';
            transcriptDiv.appendChild(finalEl);
            if (currentPartial) {
                const partialEl = document.createElement('div');
                partialEl.textContent = currentPartial;
                partialEl.style.color = '#777';
                transcriptDiv.appendChild(partialEl);
            }
            transcriptDiv.scrollTop = transcriptDiv.scrollHeight;
        }

        function stopTtsPlayback() {
            if (!playingTts) return;
            try {
                ttsAudio.pause();
                ttsAudio.removeAttribute('src');
                ttsAudio.load();
            } catch (_) {}
            if (mediaSource && mediaSource.readyState === 'open') {
                try { mediaSource.endOfStream(); } catch {}
            }
            mediaSource = null;
            sourceBuffer = null;
            sbQueue = [];
            playingTts = false;
            botState.textContent = 'Bot: idle';
        }

        async function fetchTempKey(languageHints) {
            const res = await fetch('/soniox-temp-key', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ language_hints: languageHints })
            });
            if (!res.ok) throw new Error('Failed to get temp key');
            return res.json();
        }

        function openSonioxSocketDirect(tempKey, model, languageHints, sampleRate) {
            const url = 'wss://stt-rt.soniox.com/transcribe-websocket';
            ws = new WebSocket(url);
            ws.binaryType = 'arraybuffer';
            ws.onopen = () => {
                connState.textContent = 'WS: connected';
                const config = {
                    api_key: tempKey,
                    audio_format: 'pcm_s16le',
                    sample_rate_hz: sampleRate,
                    num_channels: 1,
                    language_hints: languageHints,
                    interim_results: true
                };
                console.log('WS sending config', config);
                ws.send(JSON.stringify(config));
            };
            ws.onmessage = (event) => {
                try {
                    if (typeof event.data !== 'string') return; // ignore binary
                    const txt = event.data.trim();
                    if (!txt) return;
                    const msg = JSON.parse(txt);
                    console.log('WS message', msg);
                    // Example handling: { type: 'transcript', items: [{text, is_final}] }
                    if (msg.type === 'transcript' && Array.isArray(msg.items)) {
                        let anyFinal = false;
                        msg.items.forEach(item => {
                            if (item.is_final) {
                                finalSegments.push({ text: item.text, ts: Date.now() });
                                currentPartial = '';
                                anyFinal = true;
                            } else {
                                currentPartial = item.text;
                            }
                        });
                        renderTranscript();
                        resetEndpointTimer();
                    } else if (msg.result && Array.isArray(msg.result.words)) {
                        // Alternate schema support: accumulate words
                        let anyFinal = false;
                        const partialWords = [];
                        msg.result.words.forEach(w => {
                            if (w.is_final || w.final) {
                                finalSegments.push({ text: w.text, ts: Date.now() });
                                anyFinal = true;
                            } else {
                                partialWords.push(w.text);
                            }
                        });
                        currentPartial = partialWords.join(' ');
                        renderTranscript();
                        resetEndpointTimer();
                    } else if (msg.type === 'endpoint') {
                        const userText = finalSegments.map(s => s.text).join(' ');
                        handleUserUtterance(userText);
                        finalSegments = [];
                        currentPartial = '';
                        renderTranscript();
                    } else if (msg.type === 'error') {
                        console.error('Soniox error', msg);
                    }
                } catch (e) {
                    console.error('WS message parse error', e);
                }
            };
        function resetEndpointTimer() {
            if (endpointTimer) clearTimeout(endpointTimer);
            endpointTimer = setTimeout(() => {
                const text = finalSegments.map(s => s.text).join(' ').trim();
                if (text && text !== lastFinalSent) {
                    lastFinalSent = text;
                    handleUserUtterance(text);
                    // Keep finalSegments for transcript history; clear partial
                    currentPartial = '';
                    renderTranscript();
                }
            }, 1200);
        }

            ws.onerror = (e) => {
                console.error('WS error', e);
                connState.textContent = 'WS: error';
            };
            ws.onclose = () => {
                connState.textContent = 'WS: disconnected';
            };
        }

        async function handleUserUtterance(text) {
            if (!text || !text.trim()) return;
            // Stop any ongoing TTS (barge-in)
            stopTtsPlayback();
            botState.textContent = 'Bot: thinking…';
            // Ask backend LLM
            const res = await fetch('/respond', {
                method: 'POST',
                headers: { 'Content-Type': 'application/json' },
                body: JSON.stringify({ text })
            });
            const data = await res.json();
            const reply = data.response || '';
            const lang = data.lang || 'en';
            // Append bot text to transcript area
            finalSegments.push({ text: `\nAssistant: ${reply}`, ts: Date.now() });
            renderTranscript();
            // Request TTS stream
            botState.textContent = 'Bot: speaking…';
            // Stream playback with MediaSource for low-latency
            mediaSource = new MediaSource();
            const objUrl = URL.createObjectURL(mediaSource);
            ttsAudio.src = objUrl;
            playingTts = true;
            mediaSource.addEventListener('sourceopen', async () => {
                sourceBuffer = mediaSource.addSourceBuffer('audio/mpeg');
                sourceBuffer.mode = 'segments';
                sourceBuffer.addEventListener('updateend', () => {
                    if (sbQueue.length && !sourceBuffer.updating) {
                        sourceBuffer.appendBuffer(sbQueue.shift());
                    }
                    if (!sbQueue.length && !sourceBuffer.updating && mediaSource.readyState === 'open' && ttsDone) {
                        try { mediaSource.endOfStream(); } catch {}
                    }
                });

                let ttsDone = false;
                const res = await fetch('/tts', {
                    method: 'POST',
                    headers: { 'Content-Type': 'application/json' },
                    body: JSON.stringify({ text: reply, lang })
                });
                if (!res.ok || !res.body) {
                    botState.textContent = 'Bot: TTS error';
                    playingTts = false;
                    return;
                }
                const reader = res.body.getReader();
                const pump = async () => {
                    const { done, value } = await reader.read();
                    if (done) {
                        ttsDone = true;
                        if (!sourceBuffer.updating && mediaSource.readyState === 'open') {
                            try { mediaSource.endOfStream(); } catch {}
                        }
                        return;
                    }
                    if (value && value.byteLength) {
                        if (sourceBuffer.updating || sbQueue.length) {
                            sbQueue.push(value.buffer.slice(0));
                        } else {
                            try { sourceBuffer.appendBuffer(value.buffer); } catch (e) {
                                // On quota errors, queue and retry later
                                sbQueue.push(value.buffer.slice(0));
                            }
                        }
                    }
                    pump();
                };
                pump();
            });
            ttsAudio.onended = () => {
                playingTts = false;
                botState.textContent = 'Bot: idle';
                try { URL.revokeObjectURL(ttsAudio.src); } catch {}
            };
        }

        startBtn.addEventListener('click', async () => {
            if (isRecording) return;
            stopTtsPlayback(); // barge-in: stop bot speech immediately
            finalSegments = [];
            currentPartial = '';
            renderTranscript();
            try {
                mediaStream = await navigator.mediaDevices.getUserMedia({ audio: true });
            } catch (e) {
                alert('Microphone permission denied');
                return;
            }
            micState.textContent = 'Mic: recording';

            const selected = languageSelect.value;
            const languageHints = selected === 'auto' ? ['en','fr','nl'] : [selected];
            // Set up PCM streaming via Web Audio API using AudioWorklet
            audioCtx = new (window.AudioContext || window.webkitAudioContext)();
            const sampleRate = audioCtx.sampleRate; // typically 44100 or 48000
            // Get dev temp key/config then open Soniox WS directly
            const { temp_key } = await fetchTempKey(languageHints);
            // Use recommended RT model string
            const rtModel = 'soniox/embedded/v1/rt';
            openSonioxSocketDirect(temp_key, rtModel, language_hints, sampleRate);

            const source = audioCtx.createMediaStreamSource(mediaStream);
            try {
                await audioCtx.audioWorklet.addModule('/static/pcm-worklet.js');
                const worklet = new AudioWorkletNode(audioCtx, 'pcm-worklet', { numberOfInputs: 1, numberOfOutputs: 1, channelCount: 1 });
                worklet.port.onmessage = (event) => {
                    if (!ws || ws.readyState !== WebSocket.OPEN) return;
                    const pcm = event.data;
                    ws.send(pcm);
                };
                // connect but keep gain at 0 so it's inaudible
                const gain = audioCtx.createGain();
                gain.gain.value = 0;
                source.connect(worklet);
                worklet.connect(gain);
                gain.connect(audioCtx.destination);
            } catch (err) {
                // Fallback to deprecated ScriptProcessor if worklet fails
                console.warn('AudioWorklet unavailable, falling back to ScriptProcessor', err);
                scriptNode = audioCtx.createScriptProcessor(4096, 1, 1);
                scriptNode.onaudioprocess = (audioProcessingEvent) => {
                    if (!ws || ws.readyState !== WebSocket.OPEN) return;
                    const inputBuffer = audioProcessingEvent.inputBuffer;
                    const inputData = inputBuffer.getChannelData(0);
                    const pcmBuffer = new ArrayBuffer(inputData.length * 2);
                    const view = new DataView(pcmBuffer);
                    for (let i = 0; i < inputData.length; i++) {
                        let s = Math.max(-1, Math.min(1, inputData[i]));
                        view.setInt16(i * 2, s < 0 ? s * 0x8000 : s * 0x7fff, true);
                    }
                    ws.send(pcmBuffer);
                };
                source.connect(scriptNode);
                scriptNode.connect(audioCtx.destination);
            }

            isRecording = true;
            startBtn.disabled = true;
            stopBtn.disabled = false;
        });

        stopBtn.addEventListener('click', () => {
            if (!isRecording) return;
            isRecording = false;
            startBtn.disabled = false;
            stopBtn.disabled = true;
            micState.textContent = 'Mic: idle';
            if (ws && ws.readyState === WebSocket.OPEN) {
                try { ws.send(JSON.stringify({ eos: true })); } catch {}
                ws.close();
            }
            if (scriptNode) {
                try { scriptNode.disconnect(); } catch {}
                scriptNode = null;
            }
            if (audioCtx) {
                try { audioCtx.close(); } catch {}
                audioCtx = null;
            }
            if (mediaStream) {
                mediaStream.getTracks().forEach(t => t.stop());
                mediaStream = null;
            }
        });
    </script>
</body>
</html>
